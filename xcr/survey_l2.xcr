# submit multiple jobs by using Xcrypt

# Xcrypt specific settings
use base qw(limit core);
use File::Copy;
limit::initialize(1);# sequential run

# global settings
$id_name = "survey_l2";

# configuration of Aquarius
$GPUs_per_node = 8;
$GPUs_per_socket = 4;

# usage of Aquarius
$num_nodes = 2;
$num_procs = $num_nodes * $GPUs_per_node;

# modules on Aquarius
my $cuda = "cuda/11.1";
my $gcc = "gcc/8.3.1";
my $mpi = "ompi/4.1.1";
my $phdf5 = "phdf5/1.12.0";
my $cub = "cub/1.12.1";

# iteration counts
$num_trials = 128 / $num_procs;

# list of fixed parameters in GOTHIC
my $Ttot = 256;
my $Tsub = 32;
# my $Nloop = 4;
my $Nwarp = 1;
my $Nblck = 3;
my $use_ws = 0;

# list of parameters in GOTHIC
# my @list_l2 = (0, 1, 2, 3, 4, 5, 6, 7, 8);
my @list_l2 = (0, 4, 5, 6, 7, 8);
$num_l2 = scalar @list_l2;
# # NUNROLL <= (NLOOP * TSUB / NWARP), where NLOOP = 4, TSUB = 32, NWARP = 1
# my @list_url = (1, 2, 4, 8, 16, 32, 64, 128);
# $num_url = scalar @list_url;
my @list_wr = (1, 0);
$num_wr = scalar @list_wr;
my @list_Nloop = (3, 4);
$num_Nloop = scalar @list_Nloop;
my @list_url3 = (1, 2, 4, 8, 16, 32, 3, 6, 12, 24, 48, 96);
my @list_url4 = (1, 2, 4, 8, 16, 32, 64, 128);
$num_url3 = scalar @list_url3;
$num_url4 = scalar @list_url4;

# modules settings for Aquarius
require "/usr/share/Modules/init/perl.pm";
&module('purge');
&module('load', $cuda);
&module('load', $gcc);
&module('load', $mpi);
&module('use', '/work/gr16/share/modules/lib');
&module('load', $phdf5);
&module('load', $cub);

# compilation
my $bin = "bin";
my @target = ();
for(local $l2 = 0; $l2 < $num_l2; $l2++){
	local $l2lev = $list_l2[$l2];
	local $use_l2 = 1;
	if($l2lev == 0){
		$use_l2 = 0;
	}
	for(local $wr = 0; $wr < $num_wr; $wr++){
		local $use_wr = $list_wr[$wr];
		for(local $loop = 0; $loop < $num_Nloop; $loop++){
			local $Nloop = $list_Nloop[$loop];
			$num_url = ($Nloop == 3) ? $num_url3 : $num_url4;
			for(local $url = 0; $url < $num_url; $url++){
				# local $Nunroll = $list_url[$loop]->[$url];
                local $Nunroll = ($Nloop == 3) ? $list_url3[$url] : $list_url4[$url];

				local $exec = sprintf("l2lev%02dwr%dloop%durl%03d", $l2lev, $use_wr, $Nloop, $Nunroll);
				# `make gothic MEASURE_ELAPSED_TIME=1 HUNT_OPTIMAL_L2LEVEL=1 USE_L2SETASIDE=$use_l2 NUM_TREELEV_L2=$list_l2[$l2] ADOPT_GADGET_TYPE_MAC=1`;
				`make -j4 gothic MEASURE_ELAPSED_TIME=1 HUNT_OPTIMAL_WALK_TREE=1 HUNT_OPTIMAL_INTEGRATE=1 HUNT_OPTIMAL_MAKE_TREE=0 HUNT_OPTIMAL_MAKE_NODE=0 HUNT_OPTIMAL_NEIGHBOUR=0 HUNT_OPTIMAL_SEPARATION=1 HUNT_OPTIMAL_L2LEVEL=1 NUM_NTHREADS=$Ttot NUM_TSUB=$Tsub NUM_NLOOP=$Nloop NUM_NWARP=$Nwarp NUM_NUNROLL=$Nunroll NUM_BLOCKS_SM=$Nblck USE_WARPSHUFFLE=$use_ws USE_WARPREDUCE=$use_wr USE_L2SETASIDE=$use_l2 NUM_TREELEV_L2=$l2lev ADOPT_GADGET_TYPE_MAC=1`;

				if(-e "bin/gothic"){
					`mv bin/gothic $bin/$exec`;
					push(@target, $exec);
				}
				`make clean`;

			}
		}
	}
}
$num_target = scalar @target;

# prepare to duplicated runs
my $series = "m31";
my @file_list = ();
for(local $i = 0; $i < $num_l2; $i++){
	for(local $j = 0; $j < $num_wr; $j++){
		for(local $l = 0; $l < $num_Nloop; $l++){
            local $Nloop = $list_Nloop[$l];
			$num_url = ($Nloop == 3) ? $num_url3 : $num_url4;
			for(local $k = 0; $k < $num_url; $k++){
                local $Nunroll = ($Nloop == 3) ? $list_url3[$k] : $list_url4[$k];
				local $file = sprintf("%s_lev%d_wr%d_loop%d_url%03d", $series, $list_l2[$i], $list_wr[$j], $Nloop, $Nunroll);
				push(@file_list, $file);
				for(local $p = 0; $p < $num_procs; $p++){
					copy(sprintf("dat/%s.run.dat", $series), sprintf("dat/%s_%d.run.dat", $file, $p));
					copy(sprintf("dat/%s.cfg.dat", $series), sprintf("dat/%s_%d.cfg.dat", $file, $p));
					copy(sprintf("dat/%s.tmp0.h5", $series), sprintf("dat/%s_%d.tmp0.h5", $file, $p));
				}
			}
		}
	}
}


# Xcrypt specific settings
%template = (
	'id' => "$id_name",
	'RANGE0' => [0..$num_target-1],
	'RANGE1' => [0..$num_trials-1],

	# commands executed on compute node
	# module settings
	'exe0'  => 'module purge',
	'exe1@' => sub{"module load $cuda $gcc $mpi"},
	'exe2'  => 'module use /work/gr16/share/modules/lib',
	'exe3@' => sub{"module load $phdf5 $cub"},
	'exe4'  => 'module list',
	# job execution
	'exe5@'   => sub{"mpiexec -machinefile \$PJM_O_NODEINF -n \$PJM_MPI_PROC -npernode $GPUs_per_node sh/wisteria/split_gpu_benchmark.sh"},
	'arg5_0@' => sub{"--wrapper-Nprocs_node=$GPUs_per_node"},
	'arg5_1@' => sub{"--wrapper-Nprocs_socket=$GPUs_per_socket"},
	'arg5_2@' => sub{"--wrapper-series=$file_list[$VALUE[0]]"},
	'arg5_3'  => '--wrapper-logdir=log',
	'arg5_4@' => sub{"--wrapper-packetID=$VALUE[1]"},
	'arg5_5@' => sub{"$bin/$target[$VALUE[0]]"},
	'arg5_6'  => '-absErr=1.953125e-3',

	# commands executed before job submission
	'before' => sub{
		print "launch $self->{id}\n";
	},
	'after' => sub{
		print "finish $self->{id}\n";
	},

	# settings for job script
	'JS_queue' => 'short-a',
	'JS_phnode@' => sub{"$num_nodes"},# number of nodes
	'JS_node@' => sub{"$num_procs"},# number of MPI processes
	'JS_limit_time' => '00:10:00'
);
prepare_submit_sync(%template);
