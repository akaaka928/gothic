# submit multiple jobs by using Xcrypt

# Xcrypt specific settings
use base qw(limit core);
use File::Copy;
limit::initialize(1);# sequential run

# global settings
$id_name = "survey_l2";

# configuration of Aquarius
$GPUs_per_node = 8;
$GPUs_per_socket = 4;

# usage of Aquarius
$num_nodes = 1;
$num_procs = $num_nodes * $GPUs_per_node;

# iteration counts
$num_trials = 1024 / $num_procs;

# prepare to duplicated runs
my $series = "m31";
for(local $i = 0; $i < $num_procs; $i++){
    # cp dat/m31.* -> dat/m31_$i.*
    copy(sprintf("dat/%s.run.dat", $series), sprintf("dat/%s_%d.run.dat", $series, $i));
    copy(sprintf("dat/%s.cfg.dat", $series), sprintf("dat/%s_%d.cfg.dat", $series, $i));
    copy(sprintf("dat/%s.tmp0.h5", $series), sprintf("dat/%s_%d.tmp0.h5", $series, $i));
}

# list of parameters in GOTHIC
my @list_l2 = (0, 1, 2, 3, 4, 5, 6, 7, 8, 9);
$num_l2 = scalar @list_l2;

# modules settings for Aquarius
require "/usr/share/Modules/init/perl.pm";
&module('purge');
&module('load', 'cuda', 'gcc', 'ompi-cuda');
&module('use', '/work/gr16/share/modules/lib');
&module('load', 'phdf5', 'cub');

# compilation
my $bin = "bin";
my @target = ();
for(local $l2 = 0; $l2 < $num_l2; $l2++){
	local $exec = sprintf("l2lev%02d", $list_l2[$l2]);
	`make gothic MEASURE_ELAPSED_TIME=1 HUNT_OPTIMAL_WALK_TREE=1 HUNT_OPTIMAL_INTEGRATE=1 HUNT_OPTIMAL_MAKE_TREE=0 HUNT_OPTIMAL_MAKE_NODE=0 HUNT_OPTIMAL_NEIGHBOUR=0 HUNT_OPTIMAL_SEPARATION=1 USE_L2SETASIDE=$list_l2[$l2] ADOPT_GADGET_TYPE_MAC=1`;
	if(-e "bin/gothic"){
		`mv bin/gothic $bin/$exec`;
		push(@target, $exec);
	}
	`make clean`;
}
$num_target = scalar @target;


# Xcrypt specific settings
%template = (
	'id' => "$id_name",
	'RANGE0' => [0..$num_target-1],
	'RANGE1' => [0..$num_trials-1],

	# commands executed on compute node
	# module settings
	'exe0' => 'module purge',
	'exe1' => 'module load cuda gcc ompi-cuda',
	'exe2' => 'module use /work/gr16/share/modules/lib',
	'exe3' => 'module load phdf5 cub',
	'exe4' => 'module list',
	# job execution
	'exe5@'   => sub{"mpiexec -machinefile \$PJM_O_NODEINF -n \$PJM_MPI_PROC -npernode $GPUs_per_node sh/wisteria/split_gpu_benchmark.sh"},
	'arg5_0@' => sub{"--wrapper-Nprocs_node=$GPUs_per_node"},
	'arg5_1@' => sub{"--wrapper-Nprocs_socket=$GPUs_per_socket"},
	'arg5_2@' => sub{"--wrapper-series=$series"},
	'arg5_3'  => '--wrapper-logdir=log',
	'arg5_4@' => sub{"--wrapper-packetID=$VALUE[1]"},
	'arg5_5@' => sub{"$bin/$target[$VALUE[0]]"},
	'arg5_6'  => '-absErr=1.953125e-3',

	# commands executed before job submission
	'before' => sub{
		print "launch $self->{id}\n";
	},
	'after' => sub{
		print "finish $self->{id}\n";
	},

	# settings for job script
	'JS_queue' => 'short-a',
	'JS_phnode@' => sub{"$num_nodes"},# number of nodes
	'JS_node@' => sub{"$num_procs"}# number of MPI processes
	# 'JS_limit_time' => '00:03:00'
);
prepare_submit_sync(%template);
