# submit multiple jobs by using Xcrypt

# # Xcrypt specific settings
# use base qw(limit core);
# use File::Copy;
# limit::initialize(2);# limitation on Aquarius

# global settings
$id_name = "hunt_walk";
$num_trials = 1;

# usage of Aquarius
$num_nodes = 1;

# configuration of Aquarius
$GPUs_per_node = 8;
$GPUs_per_socket = 4;


# list up compiled binaries
my $bin = "bin";
my @target = glob('bin/tot*');
$num_target = scalar @target;

# confirmation
for(local $i = 0; $i <= $#target; $i++){
	print "$target[$i]\n";
}
print "$num_target binaries\n";

# count number of job-packets
$num_procs = $num_nodes * $GPUs_per_node;
$num_pools = (1 + int(($num_target - 1) / $num_procs));
print "$num_pools submissions\n";



# # taken from sh/slurm/compress.sh
# LIST=compress_list.txt
# if [ -e $LIST ]; then
#     rm -f $LIST
# fi
# # ls -1 $SRCDIR/*.snp???.h5 > $LIST
# ls -1 $SRCDIR/*.h5 > $LIST
# SUBLIST=compress_sub
# split -d -n l/$SLURM_NTASKS $LIST $SUBLIST
# rm -f $LIST


# # taken from sh/slurm/compress_sub.sh
# while read LINE; do
#     FILE=${LINE##*/}
#     SRC=$SRCDIR/$FILE
#     DST=$DSTDIR/$FILE
#     $NUMACTL h5repack -f SHUF -f GZIP=9 -i $SRC -o $DST
#     $NUMACTL h5diff -n 1 $SRC $DST
#     if [ $? -eq 0 ]; then
# 		# $? is 0 if h5diff returns no error
# 		$NUMACTL rm -f $SRC
# 		echo $DST | tee -a $OUT
#     else
# 		$NUMACTL rm -f $DST
# 		echo $SRC | tee -a $ERR
#     fi
# done < $LIST
# rm -f $LIST




# Xcrypt specific settings
%template = (
	'id' => "$id_name",
	'RANGE0' => [0..$num_pools-1],
	'RANGE1' => [0..$num_trials-1],

	# commands executed on compute node
	# module settings
	'exe0' => 'module purge',
	'exe1' => 'module load cuda gcc ompi-cuda',
	'exe2' => 'export MODULEPATH=/work/gr16/share/modules/lib:$MODULEPATH',
	'exe3' => 'module load phdf5 cub',
	'exe4' => 'module list',
	# job execution
	'exe5'    => 'mpiexec sh/wisteria/split_gpu.sh',
	'arg5_0@' => sub{"--wrapper-Nprocs_node=$GPUs_per_node"},
	'arg5_1@' => sub{"--wrapper-Nprocs_socket=$GPUs_per_socket"},
	# pass EXEC list via --wrapper-HOGE option
	'arg5_3@' => sub{"--wrapper-jobID-head=$VALUE[0]"},
	'arg5_4@' => sub{"--wrapper-jobID-unit=$num_procs"},# -jobID= $VALUE[0] * $num_procs + MPI_RANK
	'arg5_5'  => '-absErr=1.953125000e-3',
	'arg5_6'  => '-file=m31',
	'arg5_7'  => '--wrapper-logdir=log',
	# 'arg5_0@' => sub{"$bin/$target[$VALUE[0]]"},
	# 'arg5_3@' => sub{"-jobID=$VALUE[0]"},
	# 'arg5_4@' => sub{"1>>log/$target[$VALUE[0]]_$VALUE[0].log"},
	# 'arg5_5@' => sub{"2>>log/$target[$VALUE[0]]_$VALUE[0].err"},

	# commands executed before job submission
	'before' => sub{
		print "launch $self->{id}\n";
		# generate EXEC list (up to $num_nodes * $GPUs_per_node files)
	},
	'after' => sub{
		print "finish $self->{id}\n";
		# remove EXEC list
	},

	# settings for job script
	'JS_queue' => 'short-a',
	'JS_phnode@' => sub{"$num_nodes"},# number of nodes
	'JS_node@' => sub{"$num_procs"},# number of MPI processes
	'JS_limit_time' => '00:03:00'
);
prepare_submit_sync(%template);
